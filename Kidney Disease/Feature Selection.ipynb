{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## Import the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import os\n",
    "import imblearn\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imbfinal \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn import svm\n",
    "from sklearn.metrics import *\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import sys\n",
    "import glob\n",
    "import datetime\n",
    "import time\n",
    "import boto.s3\n",
    "from boto.s3.key import Key \n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ckd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train,df_test = train_test_split(df,train_size=0.7,random_state=42)\n",
    "x_train=df_train.iloc[:,:24]\n",
    "y_train=df_train['Classification_ckd']\n",
    "scaler.fit(x_train)\n",
    "x_train_sc=scaler.transform(x_train)\n",
    "x_test=df_test.iloc[:,:24]\n",
    "y_test=df_test['Classification_ckd']\n",
    "scaler.fit(x_test)\n",
    "x_test_sc=scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy =[]\n",
    "model_name =[]\n",
    "dataset=[]\n",
    "f1score = []\n",
    "precision = []\n",
    "recall = []\n",
    "true_positive =[]\n",
    "false_positive =[]\n",
    "true_negative =[]\n",
    "false_negative =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "## fitiing the model\n",
    "logreg.fit(x_train_sc, y_train)\n",
    "filename = 'logReg_model.sav'\n",
    "pickle.dump(logreg,open(filename,'wb'))\n",
    "logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106,   0],\n",
       "       [  0, 174]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = logreg.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "dataset.append('Training')\n",
    "model_name.append('Logistic Regression')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ -1.40593917e-01,   3.62074941e-01,  -1.90517791e+00,\n",
      "         8.14749159e-01,   5.30608385e-01,   4.69200245e-01,\n",
      "         6.44831544e-02,   7.29108238e-01,  -5.04076260e-01,\n",
      "        -2.68594976e-01,  -1.37051140e+00,  -8.11266712e-01,\n",
      "         1.85631856e-01,  -6.70832486e-01,  -3.85156898e-01,\n",
      "        -1.54641411e-03,  -2.00838374e-01,  -2.84964529e-02,\n",
      "         5.95916935e-01,   7.77114539e-01,  -4.86290514e-01,\n",
      "        -6.54114288e-01,   5.11615846e-01,  -2.06583000e-01])]\n",
      "Index(['age', 'bp', 'sg', 'al', 'su', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hemo',\n",
      "       'pcv', 'wc', 'rc', 'RBC_normal', 'PC_normal', 'PCC_present',\n",
      "       'BA_present', 'HTN_yes', 'DM_yes', 'CAD_yes', 'Appet_good', 'PE_yes',\n",
      "       'Ane_yes'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "feature_list = list(x_train.columns)\n",
    "importances = list(logreg.coef_)\n",
    "print(importances)\n",
    "print(x_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=50,random_state=0)\n",
    "## fitiing the model\n",
    "rfc.fit(x_train_sc, y_train)\n",
    "filename = 'RFC_model.sav'\n",
    "pickle.dump(rfc,open(filename,'wb'))\n",
    "rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106,   0],\n",
       "       [  0, 174]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = rfc.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Random Forest Classifier')\n",
    "dataset.append('Training')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: sc                   Importance: 0.18\n",
      "Variable: hemo                 Importance: 0.16\n",
      "Variable: pcv                  Importance: 0.14\n",
      "Variable: sg                   Importance: 0.12\n",
      "Variable: rc                   Importance: 0.11\n",
      "Variable: al                   Importance: 0.06\n",
      "Variable: DM_yes               Importance: 0.05\n",
      "Variable: bu                   Importance: 0.04\n",
      "Variable: sod                  Importance: 0.03\n",
      "Variable: bgr                  Importance: 0.02\n",
      "Variable: age                  Importance: 0.01\n",
      "Variable: bp                   Importance: 0.01\n",
      "Variable: su                   Importance: 0.01\n",
      "Variable: pot                  Importance: 0.01\n",
      "Variable: wc                   Importance: 0.01\n",
      "Variable: PC_normal            Importance: 0.01\n",
      "Variable: HTN_yes              Importance: 0.01\n",
      "Variable: Appet_good           Importance: 0.01\n",
      "Variable: PE_yes               Importance: 0.01\n",
      "Variable: RBC_normal           Importance: 0.0\n",
      "Variable: PCC_present          Importance: 0.0\n",
      "Variable: BA_present           Importance: 0.0\n",
      "Variable: CAD_yes              Importance: 0.0\n",
      "Variable: Ane_yes              Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "feature_list = list(x_train.columns)\n",
    "importances = list(rfc.feature_importances_)\n",
    "feature_importances = [(x_train, round(importance, 2)) for x_train, importance in zip(feature_list, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col_list=['RBC_normal','PCC_present','BA_present','CAD_yes','Ane_yes' , 'PC_normal' , 'Appet_good' ,\n",
    "               'age' , 'pot' , 'wc' , 'PE_yes' , 'bp' , 'su' ]\n",
    "x_train=df_train.iloc[:,:24]\n",
    "x_train.drop(drop_col_list,axis=1,inplace=True)\n",
    "y_train=df_train['Classification_ckd']\n",
    "scaler.fit(x_train)\n",
    "x_train_sc=scaler.transform(x_train)\n",
    "x_test=df_test.iloc[:,:24]\n",
    "x_test.drop(drop_col_list,axis=1,inplace=True)\n",
    "y_test=df_test['Classification_ckd']\n",
    "scaler.fit(x_test)\n",
    "x_test_sc=scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=50,random_state=0)\n",
    "## fitiing the model\n",
    "rfc.fit(x_train_sc, y_train)\n",
    "filename = 'RFC_model.sav'\n",
    "pickle.dump(rfc,open(filename,'wb'))\n",
    "rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106,   0],\n",
       "       [  0, 174]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = rfc.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Random Forest Classifier')\n",
    "dataset.append('Training')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: pcv                  Importance: 0.22\n",
      "Variable: hemo                 Importance: 0.21\n",
      "Variable: sg                   Importance: 0.13\n",
      "Variable: sc                   Importance: 0.11\n",
      "Variable: rc                   Importance: 0.09\n",
      "Variable: al                   Importance: 0.06\n",
      "Variable: DM_yes               Importance: 0.05\n",
      "Variable: bgr                  Importance: 0.03\n",
      "Variable: sod                  Importance: 0.03\n",
      "Variable: HTN_yes              Importance: 0.03\n",
      "Variable: bu                   Importance: 0.02\n"
     ]
    }
   ],
   "source": [
    "feature_list = list(x_train.columns)\n",
    "importances = list(rfc.feature_importances_)\n",
    "feature_importances = [(x_train, round(importance, 2)) for x_train, importance in zip(feature_list, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Conclusion : We have found after feature selection that these 11 columns have more importance and it's better to make the model with these."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
