{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms before feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## Import the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import os\n",
    "import imblearn\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imbfinal \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "#from sklearn import svm\n",
    "from sklearn.metrics import *\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import sys\n",
    "import glob\n",
    "import datetime\n",
    "import time\n",
    "import boto.s3\n",
    "from boto.s3.key import Key \n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('diabetes1.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(417, 8) (417,)\n"
     ]
    }
   ],
   "source": [
    "col_list=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']\n",
    "df_train,df_test = train_test_split(df,train_size=0.7,random_state=42)\n",
    "x_train=df_train[col_list]\n",
    "y_train=df_train['Outcome']\n",
    "scaler.fit(x_train)\n",
    "x_train_sc=scaler.transform(x_train)\n",
    "x_test=df_test[col_list]\n",
    "y_test=df_test['Outcome']\n",
    "scaler.fit(x_test)\n",
    "x_test_sc=scaler.transform(x_test)\n",
    "print(x_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy =[]\n",
    "model_name =[]\n",
    "dataset=[]\n",
    "f1score = []\n",
    "precision = []\n",
    "recall = []\n",
    "true_positive =[]\n",
    "false_positive =[]\n",
    "true_negative =[]\n",
    "false_negative =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "## fitiing the model\n",
    "logreg.fit(x_train_sc, y_train)\n",
    "filename = 'logReg_model.sav'\n",
    "pickle.dump(logreg,open(filename,'wb'))\n",
    "logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[193,  38],\n",
       "       [ 62, 124]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = logreg.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "dataset.append('Training')\n",
    "model_name.append('Logistic Regression')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[83, 15],\n",
       "       [26, 56]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = logreg.predict(x_test_sc)\n",
    "f1 = f1_score(y_test,  prediction)\n",
    "p = precision_score(y_test,  prediction)\n",
    "r = recall_score(y_test,  prediction)\n",
    "a = accuracy_score(y_test,  prediction)\n",
    "cm = confusion_matrix(y_test,  prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Logistc Regression')\n",
    "dataset.append('Testing')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB = BernoulliNB()\n",
    "## fitiing the model\n",
    "NB.fit(x_train_sc, y_train)\n",
    "filename = 'BernoulliNB_model.sav'\n",
    "pickle.dump(NB,open(filename,'wb'))\n",
    "NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[166,  65],\n",
       "       [ 60, 126]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = NB.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Bernoulli Naive Bayes')\n",
    "dataset.append('Training')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[73, 25],\n",
       "       [27, 55]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = NB.predict(x_test_sc)\n",
    "f1 = f1_score(y_test,  prediction)\n",
    "p = precision_score(y_test,  prediction)\n",
    "r = recall_score(y_test,  prediction)\n",
    "a = accuracy_score(y_test,  prediction)\n",
    "cm = confusion_matrix(y_test,  prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Bernoulli Naive Bayes')\n",
    "dataset.append('Testing')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=50,random_state=0)\n",
    "## fitiing the model\n",
    "rfc.fit(x_train_sc, y_train)\n",
    "filename = 'RFC_model.sav'\n",
    "pickle.dump(rfc,open(filename,'wb'))\n",
    "rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[231,   0],\n",
       "       [  0, 186]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = rfc.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Random Forest Classifier')\n",
    "dataset.append('Training')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[75, 23],\n",
       "       [19, 63]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = rfc.predict(x_test_sc)\n",
    "f1 = f1_score(y_test,  prediction)\n",
    "p = precision_score(y_test,  prediction)\n",
    "r = recall_score(y_test,  prediction)\n",
    "a = accuracy_score(y_test,  prediction)\n",
    "cm = confusion_matrix(y_test,  prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Random Forest Classifier')\n",
    "dataset.append('Testing')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(x_train_sc, y_train)\n",
    "filename = 'KNN_model.sav'\n",
    "pickle.dump(knn,open(filename,'wb'))\n",
    "knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[199,  32],\n",
       "       [ 30, 156]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = knn.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('k nearest Neighbour')\n",
    "dataset.append('Training')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[78, 20],\n",
       "       [27, 55]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = knn.predict(x_test_sc)\n",
    "f1 = f1_score(y_test,  prediction)\n",
    "p = precision_score(y_test,  prediction)\n",
    "r = recall_score(y_test,  prediction)\n",
    "a = accuracy_score(y_test,  prediction)\n",
    "cm = confusion_matrix(y_test,  prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('k nearest Neighbour')\n",
    "dataset.append('Testing')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.025, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=42, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC (kernel = 'linear' , C = 0.025 , random_state = 42)\n",
    "svc.fit(x_train_sc, y_train)\n",
    "filename = 'SVC_model.sav'\n",
    "pickle.dump(svc,open(filename,'wb'))\n",
    "svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[192,  39],\n",
       "       [ 65, 121]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = svc.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Support Vector Classifier')\n",
    "dataset.append('Training')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[83, 15],\n",
       "       [27, 55]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = svc.predict(x_test_sc)\n",
    "f1 = f1_score(y_test,  prediction)\n",
    "p = precision_score(y_test,  prediction)\n",
    "r = recall_score(y_test,  prediction)\n",
    "a = accuracy_score(y_test,  prediction)\n",
    "cm = confusion_matrix(y_test,  prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Support Vector Classifier')\n",
    "dataset.append('Testing')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='modified_huber', max_iter=None,\n",
       "       n_iter=None, n_jobs=1, penalty='l2', power_t=0.5, random_state=42,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier (loss = 'modified_huber' , shuffle = True , random_state = 42)\n",
    "sgd.fit(x_train_sc, y_train)\n",
    "filename = 'SGD_model.sav'\n",
    "pickle.dump(sgd,open(filename,'wb'))\n",
    "sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[185,  46],\n",
       "       [ 78, 108]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = sgd.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Stochastic Gradient Decent')\n",
    "dataset.append('Training')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[76, 22],\n",
       "       [33, 49]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = sgd.predict(x_test_sc)\n",
    "f1 = f1_score(y_test,  prediction)\n",
    "p = precision_score(y_test,  prediction)\n",
    "r = recall_score(y_test,  prediction)\n",
    "a = accuracy_score(y_test,  prediction)\n",
    "cm = confusion_matrix(y_test,  prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Stochastic Gradient Decent')\n",
    "dataset.append('Testing')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train_sc, y_train)\n",
    "filename = 'GNB_model.sav'\n",
    "pickle.dump(gnb,open(filename,'wb'))\n",
    "gnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[190,  41],\n",
       "       [ 70, 116]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = gnb.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Gradient Naive Bayes')\n",
    "dataset.append('Training')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[82, 16],\n",
       "       [26, 56]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = gnb.predict(x_test_sc)\n",
    "f1 = f1_score(y_test,  prediction)\n",
    "p = precision_score(y_test,  prediction)\n",
    "r = recall_score(y_test,  prediction)\n",
    "a = accuracy_score(y_test,  prediction)\n",
    "cm = confusion_matrix(y_test,  prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Gradient Naive Bayes')\n",
    "dataset.append('Testing')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing summary metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Summary = model_name,dataset,f1score,precision,recall,accuracy,true_positive,false_positive,true_negative,false_negative\n",
    "#Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Precision_score</th>\n",
       "      <th>Recall_score</th>\n",
       "      <th>Accuracy_score</th>\n",
       "      <th>True_Positive</th>\n",
       "      <th>False_Positive</th>\n",
       "      <th>True_Negative</th>\n",
       "      <th>False_Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>Training</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>231</td>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>Testing</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.732558</td>\n",
       "      <td>0.768293</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>75</td>\n",
       "      <td>23</td>\n",
       "      <td>63</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistc Regression</td>\n",
       "      <td>Testing</td>\n",
       "      <td>0.732026</td>\n",
       "      <td>0.788732</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.772222</td>\n",
       "      <td>83</td>\n",
       "      <td>15</td>\n",
       "      <td>56</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Naive Bayes</td>\n",
       "      <td>Testing</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>82</td>\n",
       "      <td>16</td>\n",
       "      <td>56</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bernoulli Naive Bayes</td>\n",
       "      <td>Testing</td>\n",
       "      <td>0.679012</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.670732</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>73</td>\n",
       "      <td>25</td>\n",
       "      <td>55</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>k nearest Neighbour</td>\n",
       "      <td>Testing</td>\n",
       "      <td>0.700637</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.670732</td>\n",
       "      <td>0.738889</td>\n",
       "      <td>78</td>\n",
       "      <td>20</td>\n",
       "      <td>55</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Support Vector Classifier</td>\n",
       "      <td>Testing</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.670732</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>83</td>\n",
       "      <td>15</td>\n",
       "      <td>55</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>k nearest Neighbour</td>\n",
       "      <td>Training</td>\n",
       "      <td>0.834225</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.851319</td>\n",
       "      <td>199</td>\n",
       "      <td>32</td>\n",
       "      <td>156</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Stochastic Gradient Decent</td>\n",
       "      <td>Testing</td>\n",
       "      <td>0.640523</td>\n",
       "      <td>0.690141</td>\n",
       "      <td>0.597561</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>76</td>\n",
       "      <td>22</td>\n",
       "      <td>49</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bernoulli Naive Bayes</td>\n",
       "      <td>Training</td>\n",
       "      <td>0.668435</td>\n",
       "      <td>0.659686</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>0.700240</td>\n",
       "      <td>166</td>\n",
       "      <td>65</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Training</td>\n",
       "      <td>0.712644</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.760192</td>\n",
       "      <td>193</td>\n",
       "      <td>38</td>\n",
       "      <td>124</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Support Vector Classifier</td>\n",
       "      <td>Training</td>\n",
       "      <td>0.699422</td>\n",
       "      <td>0.756250</td>\n",
       "      <td>0.650538</td>\n",
       "      <td>0.750600</td>\n",
       "      <td>192</td>\n",
       "      <td>39</td>\n",
       "      <td>121</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Gradient Naive Bayes</td>\n",
       "      <td>Training</td>\n",
       "      <td>0.676385</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.733813</td>\n",
       "      <td>190</td>\n",
       "      <td>41</td>\n",
       "      <td>116</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Stochastic Gradient Decent</td>\n",
       "      <td>Training</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.701299</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.702638</td>\n",
       "      <td>185</td>\n",
       "      <td>46</td>\n",
       "      <td>108</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model_Name                Dataset  F1_score  Precision_score  \\\n",
       "0     Random Forest Classifier  Training  1.000000         1.000000   \n",
       "1     Random Forest Classifier   Testing  0.750000         0.732558   \n",
       "2           Logistc Regression   Testing  0.732026         0.788732   \n",
       "3         Gradient Naive Bayes   Testing  0.727273         0.777778   \n",
       "4        Bernoulli Naive Bayes   Testing  0.679012         0.687500   \n",
       "5          k nearest Neighbour   Testing  0.700637         0.733333   \n",
       "6    Support Vector Classifier   Testing  0.723684         0.785714   \n",
       "7          k nearest Neighbour  Training  0.834225         0.829787   \n",
       "8   Stochastic Gradient Decent   Testing  0.640523         0.690141   \n",
       "9        Bernoulli Naive Bayes  Training  0.668435         0.659686   \n",
       "10         Logistic Regression  Training  0.712644         0.765432   \n",
       "11   Support Vector Classifier  Training  0.699422         0.756250   \n",
       "12        Gradient Naive Bayes  Training  0.676385         0.738854   \n",
       "13  Stochastic Gradient Decent  Training  0.635294         0.701299   \n",
       "\n",
       "    Recall_score  Accuracy_score  True_Positive  False_Positive  \\\n",
       "0       1.000000        1.000000            231               0   \n",
       "1       0.768293        0.766667             75              23   \n",
       "2       0.682927        0.772222             83              15   \n",
       "3       0.682927        0.766667             82              16   \n",
       "4       0.670732        0.711111             73              25   \n",
       "5       0.670732        0.738889             78              20   \n",
       "6       0.670732        0.766667             83              15   \n",
       "7       0.838710        0.851319            199              32   \n",
       "8       0.597561        0.694444             76              22   \n",
       "9       0.677419        0.700240            166              65   \n",
       "10      0.666667        0.760192            193              38   \n",
       "11      0.650538        0.750600            192              39   \n",
       "12      0.623656        0.733813            190              41   \n",
       "13      0.580645        0.702638            185              46   \n",
       "\n",
       "    True_Negative  False_Negative  \n",
       "0             186               0  \n",
       "1              63              19  \n",
       "2              56              26  \n",
       "3              56              26  \n",
       "4              55              27  \n",
       "5              55              27  \n",
       "6              55              27  \n",
       "7             156              30  \n",
       "8              49              33  \n",
       "9             126              60  \n",
       "10            124              62  \n",
       "11            121              65  \n",
       "12            116              70  \n",
       "13            108              78  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Making a dataframe of the accuracy and error metrics\n",
    "describe1 = pd.DataFrame(Summary[0],columns = {\"Model_Name             \"})\n",
    "describe2 = pd.DataFrame(Summary[1],columns = {\"Dataset\"})\n",
    "describe3 = pd.DataFrame(Summary[2],columns = {\"F1_score\"})\n",
    "describe4 = pd.DataFrame(Summary[3],columns = {\"Precision_score\"})\n",
    "describe5 = pd.DataFrame(Summary[4],columns = {\"Recall_score\"})\n",
    "describe6 = pd.DataFrame(Summary[5], columns ={\"Accuracy_score\"})\n",
    "describe7 = pd.DataFrame(Summary[6], columns ={\"True_Positive\"})\n",
    "describe8 = pd.DataFrame(Summary[7], columns ={\"False_Positive\"})\n",
    "describe9 = pd.DataFrame(Summary[8], columns ={\"True_Negative\"})\n",
    "describe10 = pd.DataFrame(Summary[9], columns ={\"False_Negative\"})\n",
    "des = describe1.merge(describe2, left_index=True, right_index=True, how='inner')\n",
    "des = des.merge(describe3,left_index=True, right_index=True, how='inner')\n",
    "des = des.merge(describe4,left_index=True, right_index=True, how='inner')\n",
    "des = des.merge(describe5,left_index=True, right_index=True, how='inner')\n",
    "des = des.merge(describe6,left_index=True, right_index=True, how='inner')\n",
    "des = des.merge(describe7,left_index=True, right_index=True, how='inner')\n",
    "des = des.merge(describe8,left_index=True, right_index=True, how='inner')\n",
    "des = des.merge(describe9,left_index=True, right_index=True, how='inner')\n",
    "des = des.merge(describe10,left_index=True, right_index=True, how='inner')\n",
    "#des = des.merge(describe9,left_index=True, right_index=True, how='inner')\n",
    "Summary_csv = des.sort_values(ascending=True,by=\"False_Negative\").reset_index(drop = True)\n",
    "Summary_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Summary_csv.to_csv('Summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: We are getting 100% accuracy for `Random Forest Classifier` and for every other model, we have some error which we have to rectify."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
