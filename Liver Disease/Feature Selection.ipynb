{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## Import the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import os\n",
    "import imblearn\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imbfinal \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn import svm\n",
    "from sklearn.metrics import *\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import sys\n",
    "import glob\n",
    "import datetime\n",
    "import time\n",
    "import boto.s3\n",
    "from boto.s3.key import Key \n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('liver1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(338, 8) (338,)\n"
     ]
    }
   ],
   "source": [
    "col_list=['age','tot_bilirubin','direct_bilirubin','tot_proteins','albumin','ag_ratio','sgpt'\n",
    "          ,'alkphos'\n",
    "          #,'sgot','gender_Male']\n",
    "         ]\n",
    "df_train,df_test = train_test_split(df,train_size=0.7,random_state=42)\n",
    "x_train=df_train[col_list]\n",
    "y_train=df_train['is_patient']\n",
    "scaler.fit(x_train)\n",
    "x_train_sc=scaler.transform(x_train)\n",
    "x_test=df_test[col_list]\n",
    "y_test=df_test['is_patient']\n",
    "scaler.fit(x_test)\n",
    "x_test_sc=scaler.transform(x_test)\n",
    "print(x_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy =[]\n",
    "model_name =[]\n",
    "dataset=[]\n",
    "f1score = []\n",
    "precision = []\n",
    "recall = []\n",
    "true_positive =[]\n",
    "false_positive =[]\n",
    "true_negative =[]\n",
    "false_negative =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "## fitiing the model\n",
    "logreg.fit(x_train_sc, y_train)\n",
    "filename = 'logReg_model.sav'\n",
    "pickle.dump(logreg,open(filename,'wb'))\n",
    "logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 42,  55],\n",
       "       [  6, 235]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = logreg.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "dataset.append('Training')\n",
    "model_name.append('Logistic Regression')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.47300518, -0.24585048, -0.71008588, -0.16558173, -1.07602462,\n",
      "       -1.43043243,  0.26202595, -0.03683913])]\n",
      "Index(['age', 'tot_bilirubin', 'direct_bilirubin', 'tot_proteins', 'albumin',\n",
      "       'ag_ratio', 'sgpt', 'alkphos'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "feature_list = list(x_train.columns)\n",
    "importances = list(logreg.coef_)\n",
    "print(importances)\n",
    "print(x_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=50,random_state=42)\n",
    "## fitiing the model\n",
    "rfc.fit(x_train_sc, y_train)\n",
    "filename = 'RFC_model.sav'\n",
    "pickle.dump(rfc,open(filename,'wb'))\n",
    "rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 95,   2],\n",
       "       [  0, 241]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = rfc.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Random Forest Classifier')\n",
    "dataset.append('Training')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: ag_ratio             Importance: 0.18\n",
      "Variable: direct_bilirubin     Importance: 0.14\n",
      "Variable: albumin              Importance: 0.14\n",
      "Variable: tot_proteins         Importance: 0.13\n",
      "Variable: age                  Importance: 0.12\n",
      "Variable: tot_bilirubin        Importance: 0.11\n",
      "Variable: sgpt                 Importance: 0.09\n",
      "Variable: alkphos              Importance: 0.08\n"
     ]
    }
   ],
   "source": [
    "feature_list = list(x_train.columns)\n",
    "importances = list(rfc.feature_importances_)\n",
    "feature_importances = [(x_train, round(importance, 2)) for x_train, importance in zip(feature_list, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['alkphos'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-498747348f3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdrop_col_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alkphos'\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop_col_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'is_patient'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[0;32m   2159\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2160\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2161\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2162\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2163\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   3622\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3623\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[1;32m-> 3624\u001b[1;33m                                  labels[mask])\n\u001b[0m\u001b[0;32m   3625\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3626\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: labels ['alkphos'] not contained in axis"
     ]
    }
   ],
   "source": [
    "col_list=['age','tot_bilirubin','direct_bilirubin','tot_proteins','albumin','ag_ratio','sgpt'\n",
    "          ,'alkphos'\n",
    "          #,'sgot','gender_Male']\n",
    "         ]\n",
    "df_train,df_test = train_test_split(df,train_size=0.7,random_state=42)\n",
    "x_train=df_train[col_list]\n",
    "y_train=df_train['is_patient']\n",
    "scaler.fit(x_train)\n",
    "x_train_sc=scaler.transform(x_train)\n",
    "x_test=df_test[col_list]\n",
    "y_test=df_test['is_patient']\n",
    "scaler.fit(x_test)\n",
    "x_test_sc=scaler.transform(x_test)\n",
    "print(x_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=50,random_state=0)\n",
    "## fitiing the model\n",
    "rfc.fit(x_train_sc, y_train)\n",
    "rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[288,   0],\n",
       "       [  1, 119]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = rfc.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Random Forest Classifier')\n",
    "dataset.append('Training')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: albumin              Importance: 0.16\n",
      "Variable: tot_proteins         Importance: 0.15\n",
      "Variable: ag_ratio             Importance: 0.15\n",
      "Variable: age                  Importance: 0.12\n",
      "Variable: tot_bilirubin        Importance: 0.09\n",
      "Variable: sgpt                 Importance: 0.09\n",
      "Variable: sgot                 Importance: 0.09\n",
      "Variable: direct_bilirubin     Importance: 0.08\n",
      "Variable: alkphos              Importance: 0.07\n"
     ]
    }
   ],
   "source": [
    "feature_list = list(x_train.columns)\n",
    "importances = list(rfc.feature_importances_)\n",
    "feature_importances = [(x_train, round(importance, 2)) for x_train, importance in zip(feature_list, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Conclusion : We have found after feature selection that these 11 columns have more importance and it's better to make the model with these."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
